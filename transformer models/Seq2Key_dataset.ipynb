{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run each cell in order to create a dataset of 180000 inputs and outputs and save to drive"
      ],
      "metadata": {
        "id": "fA3F-PzOnVOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6S0aOr4kSGF",
        "outputId": "3054ab6a-31cf-4b5a-c266-20453d86f30a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_nMs3EtBkKCm"
      },
      "outputs": [],
      "source": [
        "key_names = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B','Cm','C#m','Dm','D#m','Em','Fm','F#m','Gm','G#m','Am','A#m','Bm']\n",
        "N = 4\n",
        "\n",
        "def next_key(chords):\n",
        "  next = [[[0 for i in range(3)] for j in range(6)] for k in range(6)]\n",
        "  for i in range(6):\n",
        "    for j in range(6):\n",
        "      next[i][j] = chords[i][j].copy()\n",
        "      next[i][j][2] = next[i][j][2] + 1\n",
        "    if next[i][4][2] == 12:\n",
        "      next[i].pop()\n",
        "      next[i].insert(0,next[i][4].copy())\n",
        "      next[i][0][2] = next[i][0][2] - 12\n",
        "  return next\n",
        "\n",
        "def chord2token(chord):\n",
        "  return chord[0]*5*15 + chord[1]*15 + chord[2] + 1\n",
        "\n",
        "def chord2name(chord):\n",
        "  estring = ['E','F','F#','G','G#','A','A#','B','C','C#','D','D#','E','F','F#','G','A','A#']\n",
        "  astring = ['A','A#','B','C','C#','D','D#','E','F','F#','G','G#','A','A#','B','C','C#','D']\n",
        "  dstring = ['D','D#','E','F','F#','G','G#','A','A#','B','C','C#','D','D#','E','F','F#','G']\n",
        "  name = ''\n",
        "  color = chord[0]\n",
        "  shape = chord[1]\n",
        "  bar = chord[2]\n",
        "  if shape == 0: name = name + astring[bar+3]\n",
        "  elif shape == 1: name = name + astring[bar]\n",
        "  elif shape == 2: name = name + estring[bar+3]\n",
        "  elif shape == 3: name = name + estring[bar]\n",
        "  elif shape == 4: name = name + dstring[bar]\n",
        "  if color == 1: name = name + 'm'\n",
        "  return name\n",
        "\n",
        "def token2chord(token):\n",
        "  token = token - 1\n",
        "  t1 = int(token/(5*15))\n",
        "  t2 = int((token-(t1*5*15))/15)\n",
        "  t3 = token - t1*5*15 - t2*15\n",
        "  return [t1,t2,t3]\n",
        "\n",
        "def report(index,inputs,outputs):\n",
        "  for i in range(N):\n",
        "    print(chord2name(token2chord(inputs[index][i])))\n",
        "  print('key is ' + key_names[outputs[index]])\n",
        "\n",
        "def make_seq2key_dataset():\n",
        "  # specify csf chord representations\n",
        "  c = [[[0,0,0],[0,1,3],[0,2,5],[0,3,8],[0,4,10],[0,0,12]], # 1\n",
        "      [[1,4,0],[1,0,2],[1,1,5],[1,2,7],[1,3,10],[1,4,12]], # 2\n",
        "      [[1,3,0],[1,4,2],[1,0,4],[1,1,7],[1,2,9],[1,3,12]], # 3\n",
        "      [[0,3,1],[0,4,3],[0,0,5],[0,1,8],[0,2,10],[0,3,13]], # 4\n",
        "      [[0,2,0],[0,3,3],[0,4,5],[0,0,7],[0,1,10],[0,2,12]], # 5\n",
        "      [[1,1,0],[1,2,2],[1,3,5],[1,4,7],[1,0,9],[1,1,12]]] # 6\n",
        "  csharp = next_key(c)\n",
        "  d = next_key(csharp)\n",
        "  dsharp = next_key(d)\n",
        "  e = next_key(dsharp)\n",
        "  f = next_key(e)\n",
        "  fsharp = next_key(f)\n",
        "  g = next_key(fsharp)\n",
        "  gsharp = next_key(g)\n",
        "  a = next_key(gsharp)\n",
        "  asharp = next_key(a)\n",
        "  b = next_key(asharp)\n",
        "  keys = [c,csharp,d,dsharp,e,f,fsharp,g,gsharp,a,asharp,b]\n",
        "\n",
        "  # tokenize inputs\n",
        "  for i in range(len(keys)):\n",
        "    for j in range(6):\n",
        "      for k in range(6):\n",
        "        keys[i][j][k] = chord2token(keys[i][j][k].copy())\n",
        "\n",
        "  # specify progressions\n",
        "  major_progs = [[1,1,4,3],[1,3,4,6],[1,3,6,4],[1,4,2,5],[1,4,5,4],[1,4,5,5],[1,4,6,5],[1,5,1,4],[1,5,4,6],\n",
        "                    [1,5,6,2],[1,5,6,3,4,1,4,5],[1,5,6,3,4],[1,5,6,4],[1,5,6,5],[1,6,1,4],[1,6,2,4],[1,6,2,5],[1,6,4,3],\n",
        "                    [1,6,4,5],[2,4,5,5],[2,4,6,5],[2,5,1,1],[2,5,1,4],[3,6,4,1],[4,1,2,6],[4,1,3,4],[4,1,5,6],[4,4,1,5],\n",
        "                    [4,6,1,5],[4,6,3,1],[4,6,4,6],[5,1,6,5],[5,4,6,1],[5,6,4,1],[6,2,5,1],[6,4,1,5],[6,5,4,5,2,5,1,1],[6,5,4,5]]\n",
        "  minor_progs = [[6,1,2,4],[6,1,5,4],[6,2,1,4],[6,2,3,2],[6,2,3,3],[6,2,4,3],[6,2,5,6],\n",
        "                          [6,3,2,5],[6,4,1,5],[6,4,2,1],[6,4,2,3],[6,4,5,3],[6,4,5,5],[6,5,6,3,1,5,6,3,6],[6,5,6,3],\n",
        "                          [6,5,3,4],[6,5,4,1,2,4,5,6],[6,5,4,1],[6,5,4,5],[2,6,3,4],[2,1,5,6],\n",
        "                          [2,3,4,5],[2,4,3,5],[2,4,5,6],[3,6,2,5],[3,2,6,6],[3,4,1,6],[3,4,3,6],[4,6,3,1],[4,6,3,3],[4,1,6,3],\n",
        "                          [4,2,6,3],[4,4,6,5],[4,5,6,1],[4,5,3,1],[5,2,3,6],[5,2,5,6]]\n",
        "\n",
        "  # organize inputs and outputs\n",
        "  # input: sequence of N tokens in a given key\n",
        "  # output: one of 12 possible key labels\n",
        "  prog_iterations = 200\n",
        "  major_dataset = [[],[]]\n",
        "  minor_dataset = [[],[]]\n",
        "  for k in range(len(keys)):\n",
        "    key = k\n",
        "    for prog in range(len(major_progs)):\n",
        "      for iteration in range(prog_iterations):\n",
        "        sequence = [0]*N\n",
        "        for n in range(N):\n",
        "          voicing_index = random.randint(6)\n",
        "          sequence[n] = keys[key][major_progs[prog][n%len(major_progs[prog])]-1][voicing_index]\n",
        "        major_dataset[0].append(sequence)\n",
        "        major_dataset[1].append(key)\n",
        "    for prog in range(len(minor_progs)):\n",
        "      for iteration in range(prog_iterations):\n",
        "        sequence = [0]*N\n",
        "        for n in range(N):\n",
        "          voicing_index = random.randint(6)\n",
        "          sequence[n] = keys[key][minor_progs[prog][n%len(minor_progs[prog])]-1][voicing_index]\n",
        "        minor_dataset[0].append(sequence)\n",
        "        minor_dataset[1].append(key)\n",
        "  dataset = [major_dataset,minor_dataset]\n",
        "\n",
        "  # print length of dataset\n",
        "  num_sequences = len(dataset[0][0]) + len(dataset[1][0])\n",
        "  print('seq2key dataset size: ' + str(num_sequences))\n",
        "\n",
        "  # randomly \"mute\" (mask) some notes in the input sequence so the model learns how to fill in the blanks. This is directly inspired by the BERT model\n",
        "  inputs = []\n",
        "  outputs = []\n",
        "  for i in range(num_sequences):\n",
        "    color = random.randint(2)\n",
        "    if len(dataset[color][0]) == 0:\n",
        "      color = (color+1)%2\n",
        "    rand_sample_index = random.randint(len(dataset[color][0]))\n",
        "    #input_seq = mask(dataset[color][0][rand_sample_index]) # masking not currently implemented\n",
        "    input_seq = dataset[color][0][rand_sample_index].copy()\n",
        "    #output_seq = dataset[color][0][rand_sample_index] # masking not currently implemented\n",
        "    output_key = dataset[color][1][rand_sample_index]\n",
        "    inputs.append(input_seq)\n",
        "    outputs.append(output_key)\n",
        "    dataset[color][0].pop(rand_sample_index)\n",
        "    dataset[color][1].pop(rand_sample_index)\n",
        "\n",
        "  # convert inputs and outputs to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
        "  outputs = tf.convert_to_tensor(outputs, dtype=tf.int32)\n",
        "  print()\n",
        "  print('Shape of the input tensors: ' + str(tf.shape(inputs)))\n",
        "  print('Shape of the output tensors: ' + str(tf.shape(outputs)))\n",
        "  print()\n",
        "\n",
        "  # show example\n",
        "  idx = 8\n",
        "  print('Input example: ' + str(inputs[idx].numpy()))\n",
        "  print('Resulting output: ' + str(outputs[idx].numpy()))\n",
        "  print()\n",
        "  print('Example corresponds to:')\n",
        "  report(idx,inputs,outputs)\n",
        "\n",
        "  return inputs, outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = make_seq2key_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UdYFDIOmCNF",
        "outputId": "1b1390a8-417f-4c75-82b5-852d92c78b27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq2key dataset size: 180000\n",
            "\n",
            "Shape of the input tensors: tf.Tensor([180000      4], shape=(2,), dtype=int32)\n",
            "Shape of the output tensors: tf.Tensor([180000], shape=(1,), dtype=int32)\n",
            "\n",
            "Input example: [105 112  38  46]\n",
            "Resulting output: 9\n",
            "\n",
            "Example corresponds to:\n",
            "Bm\n",
            "C#m\n",
            "D\n",
            "E\n",
            "key is A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s2k_dataset_inputs = np.array(inputs)\n",
        "np.save('/content/drive/My Drive/FretboardAI/s2k_dataset_inputs',s2k_dataset_inputs)"
      ],
      "metadata": {
        "id": "pTo8G4sfmR9J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2k_dataset_outputs = np.array(outputs)\n",
        "np.save('/content/drive/My Drive/FretboardAI/s2k_dataset_outputs',s2k_dataset_outputs)"
      ],
      "metadata": {
        "id": "T2HVKIYfnIZ2"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}