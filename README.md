# Teaching AI to recognize chord progressions based on their geometric representation on a guitar fretboard

## Overview

For my project, I wanted to utilize the transformer model to classify the key of a given chord progression. From what I found online, most music analysis algorithms utilize frequency content to classify or generate music. This makes sense---I suppose in the simplest terms music can be compactly described as interesting combinations of frequencies that sound pleasing to the human ear. After all, music elements such as pitch, tone, harmony, timbre, melody, rhythm, and tempo are all described by frequency relations. However, this seems very unintuitive to me at a very fundamental level. When people learn how to play the guitar, or any intrument for that matter, they almost never bother to learn the frequencies of notes. Learning music is often geometric; we say that 'this' finger pattern sounds especially good next to 'that' finger pattern. For guitar specifically, we learn songs by reading guitar tablature (an example of guitar tab is shown in guitar_tab_example.png). A guitar chord is represented by a vector of 6 numbers, where each number represents the fret held on each of the guitar's 6 strings. I figure that a sequence of chords can be represented as an N by 6 image, where N is the number of chords in the sequence. For my project I initially thought to utilize the ViT model, since each chord could be regarded as a patch and the positional embedding would capture the order of the chords in the sequence. However, this would effectively only teach the model to remember each chord as a token, and it would give the model no insight to the relationships between notes on strings. When I learned about the swin transformer in lecture the following week, I realized that each note (as opposed to each chord) could be regarded as a patch and the different windows could be used to learn relationships across varying lengths of time and across various strings. This would allow the model to learn relationships not only between chords, but also between bass, midrange, and melodic components. My final idea was to (1) embed each note in the sequence as a patch along with an extra class patch for the key classification, (2) send the notes through 3 different windowed encoders, (3) concatenate the features of the notes from the same chord and pass through a linear layer to reduce the feature size by a factor of 6, (4) regard the chords as N patches and pass through the ViT-inspired encoder, and (5) send the result to an MLP head to classify the sequence as one of 24 possible keys (12 major and 12 minor). Unfortunately, I did not quite finish setting up the 3 windowed encoders in time, so my model consists of the embedding, the concatenation, the ViT encoder, and the MLP head. My current model failed! The loss reduced from about 12 to 8.5 in the first epoch and didn't improve from there. The model classified the key correctly once in a blue moon. I observed the output of the softmax function and it seems that the model converged to a point where every key ended up getting weighted the same, which is strange. However, it was gratifying to write a model from scratch on a custom dataset and have it train without throwing an error. I discuss the thought process behind the dataset and my ideas for improving the model below.

## The Dataset

My dataset consists of 147,600 sequences of chords represented by their guitar tablature. Each sequence consists of N=12 chords, which results in 6*12 = 72 notes in each sequence.

On the piano, musical scales are visually obvious because note frequencies gradually increase from left to right, where every 12th step doubles the frequency of the note (in music, two notes with 2:1 frequency ratio sound so pure together that we call them the same note. Because of this, for nomenclature we defined a 12 note C-C#-D-D#-E-F-F#-G-G#-A-A#-B-C pattern and repeated it continuously as frqeuencies exponentially increase. We use 12 steps because this coincidentally allows the 7th step to align with the frequency exactly halfway along the scale, the 5th step to align with the frequency exactly a third of the way along the scale, the 3rd step a quarter of the way, etc. The sharps exist because only 8 steps align with nice ratios---these are C-D-E-F-G-A-B-C. These are the common tones in music. When we change key, we change the starting note of the scale. There are 12 keys, but we often speak only in terms of the 8 main tones. Hence the term 'octave.' Chords consist of three notes in a given key with nice ratios). On the guitar, the unique tuning pattern of the strings allows for several different finger patterns to represent the same chord. Since I want my model to learn these different representations, to create my dataset I first defined the 6 common ways to represent each of the 7 common chords in each of the 24 common keys of music (12 major and 12 minor. I should note here that the minor scale is the same as the major scale started on the 6th tone. I define them differently in my dataset because chord progressions are defined as major or minor). Then, I defined 39 of the most common chord progressions in the major scale and 43 of the most common chord progressions in the minor scale. For each of these progressions in a given key, I randomly created 150 different ways to play that progression on the guitar by randomly selecting different voicings for each chord. I used 150 because this would allow my dataset to be of length 147,600 (150 voicings * (39+43) progressions * 12 keys). I liked this number because it is analogous to the size of the CIFAR dataset where there are 6,000 instances of each class (6,000 * 24 possible keys = 144,000).

I am effectively assigning a token value to each note, where the token value represents the fret number of the note on the guitar. I allocate 24 tokens to represent fret numbers, then add the <25> token to represent an unplayed string, the <26> token to represent a mask, and the <27> token for the key class. In training, I randomly mask notes with 0.05 probability. I did this because I am eventually hoping to implement a sequence-to-sequence model like BERT so the model learns how to suggest notes for songwriters.

## The Model

I have never created a model before, so I tried to more or less copy the model parameters of the transformers we saw in class. I used dim_model = 96, I ran through the ViT-inspired encoder L2 = 6 times. I plan to run through the windowed encoders L1 = 2 times once they are implemented in the future. In the colab notebook under 'Create the model' -> 'init' you can see that I have used 4.28 MB of trainable parameters, which I do not think is enough for the size of my dataset but this reduced the training time of the model. I used an ADAM optimizer. I used a batch size of 32. I used sparse categorical cross entropy loss because my labels were numbers from 1 to 24 instead of one-hot encoding. 

My ideas for the 3 windowed encoders are shown in windows.png.

## Comments

There are obviously a lot of ways I can improve this model. I think that I should try training the model without using all the different chord voicings just to see if I can get some meaningful results. Right now, with the voicings, I suspect that my datset and the number of trainable parameters in my model are too small. Once I get the key classification to work I would like to pursue the windowed encoder method to see if I improve my performance. I think the windowed implementation would improve the performance of the model when I increased the masking of the input, as the model would understand the relationships between bass, mid-range, and melodic components across individual strings. I have many many ideas for how this can be turned into a product! One idea would be to implement a decoder to convert a chord progression from one key to another, similar to the way a transformer can convert a sequence of words from one language to another. First I need to get it to work on a simple classification task though (haha). I am sure one day this will work! If I can create and classify music by only understanding the patterns of a fretboard, why can't a computer?

