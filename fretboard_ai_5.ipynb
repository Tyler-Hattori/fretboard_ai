{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WPTlsZFz25zH",
        "Li0pqB676R2B",
        "_DEaiNauk-3X"
      ],
      "authorship_tag": "ABX9TyPoiNC2KOiXK3jBMqnIJbHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyler-Hattori/fretboard_ai/blob/fourth/fretboard_ai_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "WPTlsZFz25zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oOpjpmt2yZJ",
        "outputId": "5b271f94-e6f7-4eaf-93d6-f463f315b8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy.linalg import inv\n",
        "from IPython.display import Audio, display\n",
        "from scipy.io.wavfile import read\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import torch\n",
        "from torch import nn\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization, Layer, Dense, Dropout, Flatten, Embedding, Add\n",
        "import os\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### audio feature extraction"
      ],
      "metadata": {
        "id": "Li0pqB676R2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio2tones(file_name, notes=3, play_audio=False, make_plots=False, Fs=44100):\n",
        "  # params\n",
        "  num_tones = 48\n",
        "  res = 5\n",
        "  num_tones = num_tones * res\n",
        "  dur = 1 # second\n",
        "  N = int(Fs * dur)\n",
        "  drive_path = '/content/drive/My Drive/FretboardAI/'\n",
        "  scale = ['E','F','F#','G','G#','A','A#','B','C','C#','D','D#']\n",
        "  tone_labels = [scale[np.mod(int(i/res),len(scale))] if np.mod(i,res) == 0 else ' ' for i in range(num_tones)]\n",
        "  Q = 8\n",
        "\n",
        "  # load H matrix from drive\n",
        "  H = np.load(drive_path + 'H'+str(res)+'.npy')\n",
        "\n",
        "  # store and plot custom audio file\n",
        "  sample_file_name = drive_path + 'audio/' + file_name\n",
        "\n",
        "  if play_audio:\n",
        "    # play audio\n",
        "    print('Playing audio file: ' + sample_file_name)\n",
        "    display(Audio(sample_file_name, autoplay=True))\n",
        "    print()\n",
        "\n",
        "  # extract tones\n",
        "  input_data = read(sample_file_name)\n",
        "  audio = input_data[1]\n",
        "  audio = audio[0:N]\n",
        "  audio = audio[:,0]\n",
        "  tones = H @ audio\n",
        "  tones = abs(tones)/N\n",
        "  tones = tones**2\n",
        "  tones = tones/max(tones)\n",
        "  print()\n",
        "\n",
        "  # group tones into note bins\n",
        "  tones_consolidated = np.array([tones[np.argmax(tones[i*res:i*res+res])+i*res] for i in range(int(len(tones)/res))])\n",
        "  tone_labels_consolidated = [scale[np.mod(i,len(scale))] for i in range(len(tones_consolidated))]\n",
        "\n",
        "  if make_plots:\n",
        "    # plot time signal\n",
        "    plt.figure(figsize=(20,4))\n",
        "    plt.plot(audio)\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.title(\"Audio Sample\")\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "    # plot spectrogram\n",
        "    f, t, Sxx = signal.spectrogram(audio, Fs, nfft=2048)\n",
        "    plt.figure(figsize=(20,4))\n",
        "    plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
        "    plt.ylabel('Frequency [Hz]')\n",
        "    plt.xlabel('Time [sec]')\n",
        "    plt.ylim(0, 1000) # max freq\n",
        "    plt.title('Spectrogram')\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "    # plot tones vector\n",
        "    plot = plt.figure(figsize=(20,4))\n",
        "    plt.stem(tones)\n",
        "    plt.xlabel('Tone')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Tones found in audio')\n",
        "    plt.xticks(range(len(tones)),tone_labels)\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "    # plot simplified tones vector\n",
        "    plot = plt.figure(figsize=(20,4))\n",
        "    plt.stem(tones_consolidated)\n",
        "    plt.xlabel('Tone')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Tones found in audio (max pooling)')\n",
        "    plt.xticks(range(len(tones_consolidated)),tone_labels_consolidated)\n",
        "    plt.show()\n",
        "    print()\n",
        "\n",
        "  # print max tones\n",
        "  max_tones  = tones_consolidated.argsort()[::-1][:notes]\n",
        "  notes_found = [tone_labels_consolidated[i] for i in max_tones]\n",
        "  print('Notes found are : ' + str(notes_found))\n",
        "  print()\n",
        "\n",
        "  # return simplified tones vector in binary\n",
        "  tones_binary = np.zeros(len(tones_consolidated),int)\n",
        "  for i in range(notes):\n",
        "    idx = np.argmax(tones_consolidated)\n",
        "    tones_binary[idx] = 1\n",
        "    tones_consolidated[idx] = 0\n",
        "\n",
        "  # convert binary array to a sequence of tokens. Tokens are made by converting sets of Q binary entries in tones to decimal\n",
        "  tokens = [0 for i in range(int(num_tones/(Q*res)))]\n",
        "  binaries = [pow(2,i) for i in range(Q)]\n",
        "  for i in range(int(num_tones/(Q*res))):\n",
        "    b = tones_binary[i*Q:i*Q+Q]\n",
        "    tokens[i] = np.inner(binaries,b)\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "LQQgFwT_6f2a"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer models"
      ],
      "metadata": {
        "id": "NgV0FKJ-4Fcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tones2tab"
      ],
      "metadata": {
        "id": "H0GLbKV4jpSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params\n",
        "T = 48\n",
        "Q = 8 # T/Q should be an integer\n",
        "N = int(T/Q)\n",
        "D = 96\n",
        "dff = 4*D\n",
        "H = 8\n",
        "L = 6\n",
        "dropout_rate = 0.1\n",
        "batch_size = 32\n",
        "t2t_vocab_length = pow(2,Q)\n",
        "t2t_output_dim = 25 # one token for each of 24 possible frets and another to denote the string is muted\n",
        "A = 440 # Hz\n",
        "guitar = [[A*pow(2,(-29+i)/12) for i in range(24)],\n",
        "          [A*pow(2,(-24+i)/12) for i in range(24)],\n",
        "          [A*pow(2,(-19+i)/12) for i in range(24)],\n",
        "          [A*pow(2,(-14+i)/12) for i in range(24)],\n",
        "          [A*pow(2,(-10+i)/12) for i in range(24)],\n",
        "          [A*pow(2,(-5+i)/12) for i in range(24)]]\n",
        "\n",
        "def t2t_positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class T2TPositionalEmbedding(Layer):\n",
        "  def __init__(self, vocab_size=t2t_vocab_length, dim_model=D):\n",
        "    super().__init__()\n",
        "    self.d_model = dim_model\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=dim_model, mask_zero=True)\n",
        "    self.pos_encoding = t2t_positional_encoding(length=2048, depth=dim_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    chords = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :chords, :]\n",
        "    return x\n",
        "\n",
        "class T2TBaseAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = LayerNormalization()\n",
        "    self.add = Add()\n",
        "\n",
        "class T2TGlobalSelfAttention(T2TBaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(query=x, value=x, key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    return self.layernorm(x)\n",
        "\n",
        "class T2TFeedForward(Layer):\n",
        "  def __init__(self, dim_model=D, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      Dense(dim_mlp, activation='relu'),\n",
        "      Dense(dim_model),\n",
        "      Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = Add()\n",
        "    self.layer_norm = LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class T2TEncoder(Layer):\n",
        "  def __init__(self,*, dim_model=D, num_heads=H, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.self_attention = T2TGlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=dim_model,\n",
        "        dropout=dropout_rate)\n",
        "    self.mlp = T2TFeedForward(dim_model=dim_model, dim_mlp=dim_mlp)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.mlp(x)\n",
        "    return x\n",
        "\n",
        "class Tones2Tab(tf.keras.Model):\n",
        "    def __init__(self, *, output_dim=t2t_output_dim, dim_model=D, seq_length=N, dim_mlp=dff, L=L, dropout_rate=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.L = L\n",
        "\n",
        "        self.embed = T2TPositionalEmbedding()\n",
        "        self.encoder_layers = [T2TEncoder() for _ in range(L)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        self.mlp_head1 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head2 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head3 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head4 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head5 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head6 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "\n",
        "    def call(self, x): # x is (None, N) where None refers to batch size\n",
        "        b = tf.shape(x)[0]\n",
        "        x = self.embed(x) # (None, N, dim_model)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.L): x = self.encoder_layers[i](x)\n",
        "\n",
        "        flatten = Flatten()\n",
        "        x = flatten(x) # (None, N*dim_model)\n",
        "        s1 = tf.nn.softmax(self.mlp_head1(x)) # (None, number of frets)\n",
        "        s2 = tf.nn.softmax(self.mlp_head2(x)) # (None, number of frets)\n",
        "        s3 = tf.nn.softmax(self.mlp_head3(x)) # (None, number of frets)\n",
        "        s4 = tf.nn.softmax(self.mlp_head4(x)) # (None, number of frets)\n",
        "        s5 = tf.nn.softmax(self.mlp_head5(x)) # (None, number of frets)\n",
        "        s6 = tf.nn.softmax(self.mlp_head6(x)) # (None, number of frets)\n",
        "\n",
        "        # consolidate csf outputs\n",
        "        y = []\n",
        "        for i in range(b):\n",
        "          y.append([s1[i],s2[i],s3[i],s4[i],s5[i],s6[i]])\n",
        "        out = tf.stack(y)\n",
        "\n",
        "        return out\n",
        "\n",
        "t2t = Tones2Tab()\n",
        "t2t.load_weights('/content/drive/My Drive/FretboardAI/tones2tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWuNxsRL6CoB",
        "outputId": "6b13f975-a201-47ff-ad2f-76003d3e7c81"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7be1297cb640>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tones2tab(tones,model=t2t):\n",
        "  input = tf.convert_to_tensor([tones], dtype=tf.int32)\n",
        "  soft = model(input)[0]\n",
        "  output = []\n",
        "  for i in range(len(tones)):\n",
        "    output.append(int(tf.math.argmax(soft[i])))\n",
        "  return output"
      ],
      "metadata": {
        "id": "xMHAVewklgEO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tab(tab):\n",
        "  out = [str(tab[i]-1) if tab[i] != 0 else '-' for i in range(len(tab))]\n",
        "  print('Guitar tablature: ')\n",
        "  for i in range(len(tab)):\n",
        "    print(out[i])\n",
        "  print()\n",
        "  return out"
      ],
      "metadata": {
        "id": "LsC1_c3XomrT"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tab2chord"
      ],
      "metadata": {
        "id": "_DEaiNauk-3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 6 # guitar tab input\n",
        "D = 512\n",
        "H = 8\n",
        "L = 6\n",
        "dff = D*4\n",
        "batch_size = 32\n",
        "t2c_vocab_length = 25\n",
        "t2c_output_dim = 15 # csf output\n",
        "dropout_rate = 0.1\n",
        "\n",
        "def t2c_positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class T2CPositionalEmbedding(Layer):\n",
        "  def __init__(self, vocab_size=t2c_vocab_length, dim_model=D):\n",
        "    super().__init__()\n",
        "    self.d_model = dim_model\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=dim_model, mask_zero=True)\n",
        "    self.pos_encoding = t2c_positional_encoding(length=2048, depth=dim_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    chords = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :chords, :]\n",
        "    return x\n",
        "\n",
        "class T2CBaseAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = LayerNormalization()\n",
        "    self.add = Add()\n",
        "\n",
        "class T2CGlobalSelfAttention(T2CBaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(query=x, value=x, key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    return self.layernorm(x)\n",
        "\n",
        "class T2CFeedForward(Layer):\n",
        "  def __init__(self, dim_model=D, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      Dense(dim_mlp, activation='relu'),\n",
        "      Dense(dim_model),\n",
        "      Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = Add()\n",
        "    self.layer_norm = LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class T2CEncoder(Layer):\n",
        "  def __init__(self,*, dim_model=D, num_heads=H, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.self_attention = T2CGlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=dim_model,\n",
        "        dropout=dropout_rate)\n",
        "    self.mlp = T2CFeedForward(dim_model=dim_model, dim_mlp=dim_mlp)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.mlp(x)\n",
        "    return x\n",
        "\n",
        "class Tab2Chord(tf.keras.Model):\n",
        "    def __init__(self, *, output_dim=t2c_output_dim, dim_model=D, seq_length=N, dim_mlp=dff, L=L, dropout_rate=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.L = L\n",
        "\n",
        "        self.embed = T2CPositionalEmbedding()\n",
        "        self.encoder_layers = [T2CEncoder() for _ in range(L)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        self.mlp_head1 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head2 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "        self.mlp_head3 = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "\n",
        "    def call(self, x): # x is (None, N) where None refers to batch size\n",
        "        b = tf.shape(x)[0]\n",
        "        x = self.embed(x) # (None, N, dim_model)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.L): x = self.encoder_layers[i](x)\n",
        "\n",
        "        flatten = Flatten()\n",
        "        x = flatten(x) # (None, N*dim_model)\n",
        "        c = tf.nn.softmax(self.mlp_head1(x)) # (None, number of frets)\n",
        "        s = tf.nn.softmax(self.mlp_head2(x)) # (None, number of frets)\n",
        "        f = tf.nn.softmax(self.mlp_head3(x)) # (None, number of frets)\n",
        "\n",
        "        # consolidate csf outputs\n",
        "        y = []\n",
        "        for i in range(b):\n",
        "          y.append([c[i], s[i], f[i]])\n",
        "        out = tf.stack(y)\n",
        "\n",
        "        return out\n",
        "\n",
        "t2c = Tab2Chord()\n",
        "t2c.load_weights('/content/drive/My Drive/FretboardAI/tab2chord')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y84pYFWUlJYX",
        "outputId": "7761930a-2910-4e45-f4f9-7dca9e1ba950"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7be129c7ebf0>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tab2chord(tab,model=t2c):\n",
        "  input = tf.convert_to_tensor(tab, dtype=tf.int32)\n",
        "  soft = model(input)\n",
        "  output = []\n",
        "  for i in range(len(tab)):\n",
        "    output.append([int(tf.math.argmax(soft[i][0])),int(tf.math.argmax(soft[i][1])),int(tf.math.argmax(soft[i][2]))])\n",
        "  return output"
      ],
      "metadata": {
        "id": "dEjf8UUtl5vt"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chord2token(chord):\n",
        "  return chord[0]*5*15 + chord[1]*15 + chord[2] + 1"
      ],
      "metadata": {
        "id": "waZDTieUo6bU"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chord2name(chord):\n",
        "  estring = ['E','F','F#','G','G#','A','A#','B','C','C#','D','D#','E','F','F#','G','A','A#']\n",
        "  astring = ['A','A#','B','C','C#','D','D#','E','F','F#','G','G#','A','A#','B','C','C#','D']\n",
        "  dstring = ['D','D#','E','F','F#','G','G#','A','A#','B','C','C#','D','D#','E','F','F#','G']\n",
        "  name = ''\n",
        "  color = chord[0]\n",
        "  shape = chord[1]\n",
        "  bar = chord[2]\n",
        "  if shape == 0: name = name + astring[bar+3]\n",
        "  elif shape == 1: name = name + astring[bar]\n",
        "  elif shape == 2: name = name + estring[bar+3]\n",
        "  elif shape == 3: name = name + estring[bar]\n",
        "  elif shape == 4: name = name + dstring[bar]\n",
        "  if color == 1: name = name + 'm'\n",
        "  return name"
      ],
      "metadata": {
        "id": "Eef2PJxIoRpG"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_chord(chord):\n",
        "  color = chord[0][0]\n",
        "  shape = chord[0][1]\n",
        "  bar = chord[0][2]\n",
        "  if color == 0: print('Sounds like a major chord')\n",
        "  elif color == 1: print('Sounds like a minor chord')\n",
        "  if shape == 0: print('using the C chord shape')\n",
        "  elif shape == 1: print('using the A chord shape')\n",
        "  elif shape == 2: print('using the G chord shape')\n",
        "  elif shape == 3: print('using the E chord shape')\n",
        "  elif shape == 4: print('using the D chord shape')\n",
        "  if bar != 0: print('with a capo on fret ' + str(bar))\n",
        "  print()"
      ],
      "metadata": {
        "id": "fxpqdRlfpQF-"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### seq2key"
      ],
      "metadata": {
        "id": "u-cZXCkWlvNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 4\n",
        "D = 512\n",
        "H = 8\n",
        "L = 6\n",
        "dff = D*4\n",
        "batch_size = 32\n",
        "s2k_vocab_length = 150 # possible fretboard shapes the code learns from\n",
        "s2k_output_dim = 12 # possible key labels\n",
        "dropout_rate = 0.1\n",
        "\n",
        "def s2k_positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class S2KPositionalEmbedding(Layer):\n",
        "  def __init__(self, vocab_size=s2k_vocab_length, dim_model=D):\n",
        "    super().__init__()\n",
        "    self.d_model = dim_model\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=dim_model, mask_zero=True)\n",
        "    self.pos_encoding = s2k_positional_encoding(length=2048, depth=dim_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    chords = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :chords, :]\n",
        "    return x\n",
        "\n",
        "class S2KBaseAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = LayerNormalization()\n",
        "    self.add = Add()\n",
        "\n",
        "class S2KGlobalSelfAttention(S2KBaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(query=x, value=x, key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    return self.layernorm(x)\n",
        "\n",
        "class S2KFeedForward(Layer):\n",
        "  def __init__(self, dim_model=D, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      Dense(dim_mlp, activation='relu'),\n",
        "      Dense(dim_model),\n",
        "      Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = Add()\n",
        "    self.layer_norm = LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class S2KEncoder(Layer):\n",
        "  def __init__(self,*, dim_model=D, num_heads=H, dim_mlp=dff, dropout_rate=dropout_rate):\n",
        "    super().__init__()\n",
        "    self.self_attention = S2KGlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=dim_model,\n",
        "        dropout=dropout_rate)\n",
        "    self.mlp = S2KFeedForward(dim_model=dim_model, dim_mlp=dim_mlp)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.mlp(x)\n",
        "    return x\n",
        "\n",
        "class Seq2Key(tf.keras.Model):\n",
        "    def __init__(self, *, output_dim=s2k_output_dim, dim_model=D, seq_length=N, dim_mlp=dff, L=L, dropout_rate=dropout_rate):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.L = L\n",
        "\n",
        "        self.embed = S2KPositionalEmbedding()\n",
        "        self.encoder_layers = [S2KEncoder() for _ in range(L)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        self.mlp_head = tf.keras.Sequential([\n",
        "          Dense(dim_mlp, activation='relu'),\n",
        "          Dense(output_dim)\n",
        "        ])\n",
        "\n",
        "    def call(self, x): # x is (None, N) where None refers to batch size\n",
        "        x = self.embed(x) # (None, N, dim_model)\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.L): x = self.encoder_layers[i](x)\n",
        "\n",
        "        flatten = Flatten()\n",
        "        x = flatten(x) # (None, N*dim_model)\n",
        "        pred = self.mlp_head(x) # (None, 12)\n",
        "\n",
        "        pred = tf.nn.softmax(pred)\n",
        "\n",
        "        return pred\n",
        "\n",
        "s2k = Seq2Key()\n",
        "s2k.load_weights('/content/drive/My Drive/FretboardAI/seq2key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHuJ0xKglzcA",
        "outputId": "c90d647c-ca09-4752-fb6e-6535fd048a80"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7be1134e6020>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2key(seq,model=s2k):\n",
        "  if len(seq) > 4:\n",
        "    seq = seq[-4:]\n",
        "  elif len(seq) < 4:\n",
        "    for i in range(4-len(seq)): seq.append(seq[np.mod(i,len(seq))])\n",
        "  input = tf.convert_to_tensor([seq], dtype=tf.int32)\n",
        "  soft = model(input)[0]\n",
        "  key_names = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B','Cm','C#m','Dm','D#m','Em','Fm','F#m','Gm','G#m','Am','A#m','Bm']\n",
        "  return key_names[np.argmax(soft)]"
      ],
      "metadata": {
        "id": "uSOx1e0IjUxv"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main audio conversion function"
      ],
      "metadata": {
        "id": "6Rt2kxEC6HXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(input,output='tab',make_plots=False,play_audio=False):\n",
        "  # audio frequency component extraction\n",
        "  tones_seq = audio2tones(input,play_audio=play_audio,make_plots=make_plots)\n",
        "\n",
        "  # convert frequency content to reasonable guitar tablature\n",
        "  tab = tones2tab(tones_seq)\n",
        "  neat_tab = print_tab(tab)\n",
        "\n",
        "  # express guitar tablature as a sequence of fretboard patterns\n",
        "  chord = tab2chord([tab])\n",
        "  print_chord(chord)\n",
        "\n",
        "  # convert the sequence of fretboard patterns to a sequence of tokens\n",
        "  seq = [] # chord tokens\n",
        "  prog = [] # chord names\n",
        "  for i in range(len(chord)):\n",
        "    chord_name = chord2name(chord[i])\n",
        "    print('Chord '+str(i+1)+': ' + chord_name)\n",
        "    prog.append(chord_name)\n",
        "    seq.append(chord2token(chord[i]))\n",
        "  print()\n",
        "\n",
        "  # classify the sequence of tokens as one of 12 musical keys\n",
        "  key = seq2key(seq)\n",
        "  print('Sounds like the key of ' + key)\n",
        "\n",
        "  return neat_tab, chord, prog, key"
      ],
      "metadata": {
        "id": "UQh341n96JQF"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing"
      ],
      "metadata": {
        "id": "5gi044M24Ht7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tab, pattern, chord, key = convert('-354--.wav',play_audio=False,make_plots=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMoZ25go6DTs",
        "outputId": "268b7c30-e561-4958-ab2e-a0bdfebf3878"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-151-65b71fa438a0>:26: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  input_data = read(sample_file_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Notes found are : ['B', 'B', 'F#']\n",
            "\n",
            "Guitar tablature: \n",
            "-\n",
            "2\n",
            "4\n",
            "4\n",
            "-\n",
            "-\n",
            "\n",
            "Sounds like a minor chord\n",
            "using the A chord shape\n",
            "with a capo on fret 2\n",
            "\n",
            "Chord 1: Bm\n",
            "\n",
            "Sounds like the key of D\n"
          ]
        }
      ]
    }
  ]
}